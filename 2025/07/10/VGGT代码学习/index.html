<!DOCTYPE html>
<html lang="en">

<head>

  <!-- Minima -->
  <!-- Hexo theme created by @adisaktijrs -->

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">

  
  <title>VGGT代码学习</title>
  
  <link rel="canonical" href="http://example.com/2025/07/10/VGGT%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0/">
  
  <meta name="description" content="VGGT: Visual Geometry Grounded Transformer （CVPR 2025 Best Paper）Code | Paper | Suplementary 1. 方法介绍 VGGT是一种创新的前馈神经网络，能够直接从单张、少量或上百张视图中推断出场景的全部关键3D属性，">
  
  
  <meta name="author" content="Malong">
  
  
  
  <meta property="og:site_name" content="Malong&#39;s Blog" />
  <meta property="og:type" content="article" />
  <meta property="og:title" content="VGGT代码学习" />
  
  <meta property="og:description" content="VGGT: Visual Geometry Grounded Transformer （CVPR 2025 Best Paper）Code | Paper | Suplementary 1. 方法介绍 VGGT是一种创新的前馈神经网络，能够直接从单张、少量或上百张视图中推断出场景的全部关键3D属性，">
  
  <meta property="og:url" content="http://example.com/2025/07/10/VGGT%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0/" />

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="VGGT代码学习">
  
  <meta name="twitter:description" content="VGGT: Visual Geometry Grounded Transformer （CVPR 2025 Best Paper）Code | Paper | Suplementary 1. 方法介绍 VGGT是一种创新的前馈神经网络，能够直接从单张、少量或上百张视图中推断出场景的全部关键3D属性，">
  
  
  
  
  <meta name="twitter:url" content="http://example.com/2025/07/10/VGGT%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0/" />

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Preload fonts
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="preload" href="/fonts/dm-serif-display-v4-latin-regular.woff2" as="font" type="font/woff2" crossorigin>
  <link rel="preload" href="/fonts/inter-v2-latin-regular.woff2" as="font" type="font/woff2" crossorigin>

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  
<link rel="stylesheet" href="/css/normalize.css">

  
<link rel="stylesheet" href="/css/skeleton.css">

  
<link rel="stylesheet" href="/css/custom.css">

  
<link rel="stylesheet" href="/css/prism-dark.css">

  
<link rel="stylesheet" href="/css/prism-line-numbers.css">

  <!-- User css -->
  

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href="/images/favicon.png">

  <!-- Custom Theme Color Style
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <style>
  a:not(.icon) {
    text-decoration-color: #0FA0CE;
    background-image: linear-gradient(
      to bottom,
      rgba(0, 0, 0, 0) 50%,
      #0FA0CE 50%
    );
  }
  blockquote {
    border-left: 8px solid #0FA0CE;
  }
  .nanobar .bar {
    background: #0FA0CE;
  }
  .button.button-primary:hover,
  button.button-primary:hover,
  input[type="submit"].button-primary:hover,
  input[type="reset"].button-primary:hover,
  input[type="button"].button-primary:hover,
  .button.button-primary:focus,
  button.button-primary:focus,
  input[type="submit"].button-primary:focus,
  input[type="reset"].button-primary:focus,
  input[type="button"].button-primary:focus {
    background-color: #0FA0CE;
    border-color: #0FA0CE;
  }
  input[type="email"]:focus,
  input[type="number"]:focus,
  input[type="search"]:focus,
  input[type="text"]:focus,
  input[type="tel"]:focus,
  input[type="url"]:focus,
  input[type="password"]:focus,
  textarea:focus,
  select:focus {
    border: 1px solid #0FA0CE;
  }
</style>

  <!-- Google Analytics (With Privacy Settings On)
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  

  
  <script src="/js/pic.min.js" defer></script>
  

  

<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div class="container">
    <div class="row">
      <div>

        <div class="row">
  <div class="two columns" style="max-width: 50px">
    <h1 class="mt-2 mode">
      <div onclick=setDarkMode(true) id="darkBtn">🌑</div>
      <div onclick=setDarkMode(false) id="lightBtn" class=hidden>☀️</div>
      <script >
        if (localStorage.getItem('preferredTheme') == 'dark') {
          setDarkMode(true)
        }
        function setDarkMode(isDark) {
          var darkBtn = document.getElementById('darkBtn')
          var lightBtn = document.getElementById('lightBtn')
          if (isDark) {
            lightBtn.style.display = "block"
            darkBtn.style.display = "none"
            localStorage.setItem('preferredTheme', 'dark');
          } else {
            lightBtn.style.display = "none"
            darkBtn.style.display = "block"
            localStorage.removeItem('preferredTheme');
          }
          document.body.classList.toggle("darkmode");
        }
      </script>
    </h1>
  </div>

  <div class="six columns ml-1">
    <h1 class="mt-2">
      Malong&#39;s Blog
    </h1>
  </div>

  <div class="twelve columns">
    <div class="row">
      <div class="nine columns left">
        <a href="/">Home</a>
        
          
          <a href="/Works" class="ml">Works</a>
          
        
        
          
            <a href="mailto:1289596706@qq.com" target="_blank" class="ml">Email</a>
          
        
      </div>
    </div>
    <hr style="margin-bottom: 2.6rem">
  </div>
</div>

        <div class="trans">
            <h2>VGGT代码学习</h2>

  <h1 id="VGGT-Visual-Geometry-Grounded-Transformer-（CVPR-2025-Best-Paper）"><a href="#VGGT-Visual-Geometry-Grounded-Transformer-（CVPR-2025-Best-Paper）" class="headerlink" title="VGGT: Visual Geometry Grounded Transformer （CVPR 2025 Best Paper）"></a>VGGT: Visual Geometry Grounded Transformer （CVPR 2025 Best Paper）</h1><p><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/vggt">Code</a> | <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_VGGT_Visual_Geometry_Grounded_Transformer_CVPR_2025_paper.pdf">Paper</a> | <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2025/supplemental/Wang_VGGT_Visual_Geometry_CVPR_2025_supplemental.pdf">Suplementary</a></p>
<h2 id="1-方法介绍"><a href="#1-方法介绍" class="headerlink" title="1. 方法介绍"></a>1. 方法介绍</h2><p><img src="/../images/image-1.png" alt="VGGT架构图"></p>
<p>VGGT是一种创新的前馈神经网络，能够直接从单张、少量或上百张视图中推断出场景的全部关键3D属性，包括相机参数、点图、深度图和3D点轨迹。</p>
<p>如上图所示，VGGT首先利用DINOv2从输入图像中提取Patch Tokens，并为每张图片添加一个随机初始化的Camera Token和Register Tokens。随后，将所有Patch Tokens、Register Tokens和Camera Token拼接，形成完整的输入序列。该序列被送入由Frame Attention和Global Attention组成的 交替注意力（Alternating-Attention，AA） 结构进行处理：Frame Attention专注于单张图像内部的Tokens，Global Attention则实现跨帧特征交互，关注所有图像间的Tokens。</p>
<p>在相机参数预测方面，模型通过4层额外的自注意力层和一个线性层，输出相机的内参和外参，参数化为旋转四元数、平移向量和视场角，统一为9维向量。对于点图和深度图，模型借助<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Ranftl_Vision_Transformers_for_Dense_Prediction_ICCV_2021_paper.pdf">DPT</a>层将特征转换为密集特征图，再通过3×3和1×1卷积层输出深度图&#x2F;点图及其不确定性。3D点轨迹的预测则采用CoTracker2架构，将DPT提取的密集跟踪特征作为输入。具体而言，给定查询图像中的查询点，跟踪头能够预测所有帧中与该查询点对应的点集，实现高效的跨帧点追踪。</p>
<h2 id="2-代码学习"><a href="#2-代码学习" class="headerlink" title="2. 代码学习"></a>2. 代码学习</h2><p><img src="/../images/image-2.png" alt="VGGT类"></p>
<h3 id="2-1-Aggregator"><a href="#2-1-Aggregator" class="headerlink" title="2.1 Aggregator"></a>2.1 Aggregator</h3><p>Aggregator类是VGGT的核心组件，处理输入的多视角图片，提取特征并使用交替注意力来处理这些特征。</p>
<p>该模块可以分为3个主要部分：</p>
<ul>
<li>特征提取：使用DINOv2模型从输入图像中提取Patch Tokens，并且添加一个随机初始化的Camera Token和4个Register Token得到全部的特征序列。</li>
<li>位置编码：对特征序列使用RoPE生成Patch Tokens的二维位置编码，并为特殊Tokens生成0维位置编码。</li>
<li>交替注意力：使用Frame Attention和Global Attention来处理特征序列。（先进行Frame Attention之后进行Global Attention）两种注意力的实现都使用了RoPE位置编码。</li>
</ul>
<p>以下是<strong>Aggregator</strong>类的详细流程说明，结合源码实现：</p>
<hr>
<h4 id="Aggregator-的前向传播"><a href="#Aggregator-的前向传播" class="headerlink" title="Aggregator 的前向传播"></a>Aggregator 的前向传播</h4><h4 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h4><ul>
<li><code>images</code>：形状 <code>[B, S, 3, H, W]</code>，像素归一化到 [0, 1]，其中<ul>
<li><code>B</code> 是 batch size，</li>
<li><code>S</code> 是帧数，</li>
<li><code>3</code> 是 RGB 通道，</li>
<li><code>H</code> 和 <code>W</code> 分别是图片的高度和宽度。</li>
</ul>
</li>
</ul>
<h4 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h4><ol>
<li><p><strong>归一化</strong>  </p>
<ul>
<li>用 ImageNet 均值和方差对输入图片归一化。</li>
</ul>
</li>
<li><p><strong>DINOv2特征提取</strong>  </p>
<ul>
<li>将图片 reshape 为 <code>[B*S, 3, H, W]</code>，输入 DINOv2中，得到 patch tokens，形状 <code>[B*S, P, C]</code>。</li>
<li>其中 <code>P</code> 是每帧的 patch 数量，<code>C</code> 是嵌入维度。</li>
</ul>
</li>
<li><p><strong>特殊 token 拼接</strong>  </p>
<ul>
<li>camera_token 和 register_token 通过 <code>slice_expand_and_flatten</code> 展开到 <code>[B*S, 1, C]</code> 和 <code>[B*S, num_register_tokens, C]</code>。</li>
<li>与 patch tokens 拼接，得到 <code>[B*S, 1+num_register_tokens+P, C]</code>。</li>
</ul>
</li>
<li><p><strong>位置编码</strong>  </p>
<ul>
<li>若启用 RoPE，生成 patch token 的二维位置编码，并为特殊 token 补零。</li>
</ul>
</li>
<li><p><strong>交替注意力堆叠</strong>  </p>
<ul>
<li>交替执行 frame attention 和 global attention，每种 attention 连续堆叠 aa_block_size 层。</li>
<li>frame attention：只在每帧内部做自注意力（tokens 形状 <code>[B*S, P, C]</code>）。</li>
<li>global attention：在所有帧的 token 间做自注意力（tokens reshape 为 <code>[B, S*P, C]</code>）。</li>
<li>每次堆叠后，frame 和 global attention 的输出分别保存为 intermediates。</li>
<li>每一组交替 attention 后，将 frame 和 global 的输出在最后一维拼接（<code>[B, S, P, 2C]</code>），加入 output_list。</li>
</ul>
</li>
<li><p><strong>输出</strong>  </p>
<ul>
<li>返回所有交替 attention 层的输出列表（每层形状 <code>[B, S, P, 2C]</code>），以及 patch_start_idx。</li>
</ul>
</li>
</ol>
<h3 id="2-2-Camera-Head"><a href="#2-2-Camera-Head" class="headerlink" title="2.2 Camera Head"></a>2.2 Camera Head</h3><p>Camera Head 负责从 Aggregator 输出的 token 表达中，预测每一帧的相机参数（如平移向量、旋转矩阵、视场角等），并采用迭代细化的方式提升预测精度。</p>
<hr>
<h3 id="Camera-Head-的前向传播"><a href="#Camera-Head-的前向传播" class="headerlink" title="Camera Head 的前向传播"></a>Camera Head 的前向传播</h3><h4 id="输入-1"><a href="#输入-1" class="headerlink" title="输入"></a>输入</h4><ul>
<li><code>aggregated_tokens_list</code>：Aggregator 输出的 token 列表，取最后一层用于相机预测。</li>
<li><code>num_iterations</code>：迭代细化次数（默认 4）。</li>
</ul>
<h4 id="步骤-1"><a href="#步骤-1" class="headerlink" title="步骤"></a>步骤</h4><ol>
<li><p><strong>提取 camera token</strong>  </p>
<ul>
<li>从最后一层 token 中提取 camera token（通常为每帧的第一个 token），形状 <code>[B, S, C]</code>。</li>
<li>经过 LayerNorm 归一化。</li>
</ul>
</li>
<li><p><strong>迭代细化（<code>trunk_fn</code>）</strong>  </p>
<ul>
<li>初始化 pred_pose_enc&#x3D;None。</li>
<li>进行 num_iterations 次循环，每次细化预测：<ol>
<li><strong>第一次</strong>：用 empty_pose_tokens 经过 embed_pose 作为输入。</li>
<li><strong>后续</strong>：用上一次预测的 pose 编码（detach 后）经过 embed_pose 作为输入。</li>
<li><strong>生成 modulation 参数</strong>：poseLN_modulation 输出 shift、scale、gate。</li>
<li><strong>自适应归一化和调制</strong>：对 camera token 做 adaln_norm，然后用 shift&#x2F;scale&#x2F;gate 调制（modulate）。</li>
<li><strong>主干网络处理</strong>：调制后的 token 输入 trunk（Transformer Block）。</li>
<li><strong>MLP 输出 delta</strong>：trunk_norm 后输入 pose_branch，得到本次的 pose delta。</li>
<li><strong>累加</strong>：pred_pose_enc &#x3D; pred_pose_enc + delta。</li>
<li><strong>激活</strong>：对平移向量、四元数、FOV 分别用指定激活函数处理，得到最终 pose 编码。</li>
<li><strong>保存</strong>：每次迭代的 pose 编码都保存到列表。</li>
</ol>
</li>
</ul>
</li>
<li><p><strong>输出</strong>  </p>
<ul>
<li>返回每次迭代的 pose 编码列表。</li>
</ul>
</li>
</ol>
<hr>
<h4 id="关键点说明"><a href="#关键点说明" class="headerlink" title="关键点说明"></a>关键点说明</h4><ul>
<li><strong>迭代 refinement</strong>：通过多次 refinement，逐步细化相机参数预测，提升精度和稳定性。</li>
<li><strong>自适应归一化与调制</strong>：借鉴 DiT 等模型，用 shift&#x2F;scale&#x2F;gate 对 token 做调制，增强表达能力。</li>
<li><strong>激活函数</strong>：对不同类型参数采用不同激活，保证物理合理性（如 FOV 必须为正，四元数归一化等）。</li>
</ul>
<h3 id="2-3-Depth-Head"><a href="#2-3-Depth-Head" class="headerlink" title="2.3 Depth Head"></a>2.3 Depth Head</h3><p>Depth Head用于从 Aggregator 输出的 token 表达中，预测每一帧的深度图和不确定性。</p>
<hr>
<h4 id="输入-2"><a href="#输入-2" class="headerlink" title="输入"></a>输入</h4><ul>
<li><code>aggregated_tokens_list</code>: Aggregator输出的多层token特征列表，每层形状为 <code>[B, S, P + 1 + num_register_tokens, 2*C]</code></li>
<li><code>images</code>: 原始输入图像，形状为 <code>[B, S, 3, H, W]</code>，像素值范围 [0, 1]</li>
<li><code>patch_start_idx</code>: patch token起始索引，通常为 <code>1 + num_register_tokens</code></li>
</ul>
<h4 id="步骤-2"><a href="#步骤-2" class="headerlink" title="步骤"></a>步骤</h4><h4 id="1-多层特征提取（4个中间层）"><a href="#1-多层特征提取（4个中间层）" class="headerlink" title="1. 多层特征提取（4个中间层）"></a>1. 多层特征提取（4个中间层）</h4><p>对于 <code>intermediate_layer_idx</code> 指定的4个层（默认为第4、11、17、23层），依次进行以下处理：</p>
<h5 id="1-1-提取patch-token"><a href="#1-1-提取patch-token" class="headerlink" title="1.1 提取patch token"></a>1.1 提取patch token</h5><ul>
<li>从 <code>aggregated_tokens_list[layer_idx]</code> 中提取patch部分：<code>[:, :, patch_start_idx:]</code></li>
<li>输入形状：<code>[B, S, P + 1 + num_register_tokens, 2*C]</code></li>
<li>输出形状：<code>[B, S, P, 2*C]</code></li>
</ul>
<h5 id="1-2-重新组织张量"><a href="#1-2-重新组织张量" class="headerlink" title="1.2 重新组织张量"></a>1.2 重新组织张量</h5><ul>
<li>将batch和序列维度合并：<code>reshape(B * S, P, 2*C)</code></li>
<li>对token进行LayerNorm归一化</li>
<li>重排为空间格式：<code>(B*S, 2*C, patch_h, patch_w)</code><ul>
<li><code>patch_h = H // patch_size</code></li>
<li><code>patch_w = W // patch_size</code></li>
</ul>
</li>
</ul>
<h5 id="1-3-通道投影"><a href="#1-3-通道投影" class="headerlink" title="1.3 通道投影"></a>1.3 通道投影</h5><ul>
<li>通过1x1卷积投影到指定通道数：<code>projects[dpt_idx](x)</code></li>
<li>输出形状：<code>[B*S, out_channels[dpt_idx], patch_h, patch_w]</code><ul>
<li>其中 out_channels &#x3D; [256, 512, 1024, 1024]</li>
</ul>
</li>
</ul>
<h5 id="1-4-UV位置编码"><a href="#1-4-UV位置编码" class="headerlink" title="1.4 UV位置编码"></a>1.4 UV位置编码</h5><ul>
<li>创建UV网格：<code>[patch_h, patch_w, 2]</code></li>
<li>转换为位置embedding：<code>[patch_h, patch_w, out_channels[dpt_idx]]</code></li>
<li>与特征相加：<code>x + pos_embed</code></li>
</ul>
<h5 id="1-5-多尺度空间变换"><a href="#1-5-多尺度空间变换" class="headerlink" title="1.5 多尺度空间变换"></a>1.5 多尺度空间变换</h5><p>通过 <code>resize_layers</code> 生成4个不同尺度的特征：</p>
<ul>
<li><strong>Layer 0</strong>: ConvTranspose2d(kernel&#x3D;4, stride&#x3D;4) → <code>[B*S, 256, patch_h*4, patch_w*4]</code></li>
<li><strong>Layer 1</strong>: ConvTranspose2d(kernel&#x3D;2, stride&#x3D;2) → <code>[B*S, 512, patch_h*2, patch_w*2]</code>  </li>
<li><strong>Layer 2</strong>: Identity() → <code>[B*S, 1024, patch_h, patch_w]</code></li>
<li><strong>Layer 3</strong>: Conv2d(kernel&#x3D;3, stride&#x3D;2) → <code>[B*S, 1024, patch_h//2, patch_w//2]</code></li>
</ul>
<h4 id="2-特征融合（scratch-forward）"><a href="#2-特征融合（scratch-forward）" class="headerlink" title="2. 特征融合（scratch forward）"></a>2. 特征融合（scratch forward）</h4><h5 id="2-1-通道统一"><a href="#2-1-通道统一" class="headerlink" title="2.1 通道统一"></a>2.1 通道统一</h5><p>将4个尺度的特征通过3x3卷积（<strong>纯卷积</strong>）统一到features通道（256）：</p>
<ul>
<li>layer_1_rn: <code>[B*S, 256, patch_h*4, patch_w*4]</code></li>
<li>layer_2_rn: <code>[B*S, 256, patch_h*2, patch_w*2]</code></li>
<li>layer_3_rn: <code>[B*S, 256, patch_h, patch_w]</code></li>
<li>layer_4_rn: <code>[B*S, 256, patch_h//2, patch_w//2]</code></li>
</ul>
<h5 id="2-2-自底向上融合"><a href="#2-2-自底向上融合" class="headerlink" title="2.2 自底向上融合"></a>2.2 自底向上融合</h5><p>特征融合采用FeatureFusionBlock模块，逐层自底向上整合多尺度特征。该模块支持跳跃连接（skip connection），以增强不同层级特征的交互。</p>
<ul>
<li><strong>不使用跳跃连接</strong>：仅输入x1，经过两层带残差的卷积后直接上采样。</li>
<li><strong>使用跳跃连接</strong>：输入为x1和x2。首先对x1进行两层带残差的卷积处理，然后将x2作为残差项加到x1上。之后再次通过两层带残差的卷积，最后进行上采样操作。</li>
</ul>
<p>具体流程如下：</p>
<ol>
<li><p><strong>refinenet4</strong>: 处理最小尺度特征，没有跳跃连接</p>
<ul>
<li>输入：layer_4_rn <code>[B*S, 256, patch_h//2, patch_w//2]</code></li>
<li>上采样到layer_3尺度：<code>[B*S, 256, patch_h, patch_w]</code></li>
</ul>
</li>
<li><p><strong>refinenet3</strong>: 融合第3层，</p>
<ul>
<li>输入：上步输出 + layer_3_rn</li>
<li>上采样到layer_2尺度：<code>[B*S, 256, patch_h*2, patch_w*2]</code></li>
</ul>
</li>
<li><p><strong>refinenet2</strong>: 融合第2层</p>
<ul>
<li>输入：上步输出 + layer_2_rn  </li>
<li>上采样到layer_1尺度：<code>[B*S, 256, patch_h*4, patch_w*4]</code></li>
</ul>
</li>
<li><p><strong>refinenet1</strong>: 融合第1层</p>
<ul>
<li>输入：上步输出 + layer_1_rn</li>
<li>最终上采样：<code>[B*S, 256, patch_h*8, patch_w*8]</code></li>
</ul>
</li>
</ol>
<p>这种自底向上的多层级融合方式，能够充分整合不同语义层次的特征信息，有效提升深度预测的空间细节和全局一致性。</p>
<h4 id="2-3-第一层输出卷积"><a href="#2-3-第一层输出卷积" class="headerlink" title="2.3 第一层输出卷积"></a>2.3 第一层输出卷积</h4><ul>
<li><code>output_conv1</code>: 3x3卷积，输出通道减半</li>
<li>输出形状：<code>[B*S, 128, patch_h*8, patch_w*8]</code></li>
</ul>
<h4 id="3-空间分辨率恢复"><a href="#3-空间分辨率恢复" class="headerlink" title="3. 空间分辨率恢复"></a>3. 空间分辨率恢复</h4><ul>
<li>使用双线性插值上采样到目标分辨率：</li>
<li>输入：<code>[B*S, 128, patch_h*8, patch_w*8]</code></li>
<li>输出：<code>[B*S, 128, H//down_ratio, W//down_ratio]</code>（通常down_ratio&#x3D;1）</li>
</ul>
<h4 id="4-再次位置编码（可选）"><a href="#4-再次位置编码（可选）" class="headerlink" title="4. 再次位置编码（可选）"></a>4. 再次位置编码（可选）</h4><p>如果启用位置编码，再次添加UV位置编码增强空间感知</p>
<h4 id="5-最终输出处理"><a href="#5-最终输出处理" class="headerlink" title="5. 最终输出处理"></a>5. 最终输出处理</h4><h5 id="5-1-第二层输出卷积"><a href="#5-1-第二层输出卷积" class="headerlink" title="5.1 第二层输出卷积"></a>5.1 第二层输出卷积</h5><ul>
<li><code>output_conv2</code>: 包含3x3卷积 + ReLU + 1x1卷积</li>
<li>输出2个通道：<code>[B*S, 2, H//down_ratio, W//down_ratio]</code></li>
</ul>
<h5 id="5-2-激活函数处理"><a href="#5-2-激活函数处理" class="headerlink" title="5.2 激活函数处理"></a>5.2 激活函数处理</h5><p>通过 <code>activate_head</code> 分离并激活两个通道：</p>
<ul>
<li><strong>深度通道</strong>: 使用”exp”激活 → <code>[B*S, H//down_ratio, W//down_ratio, 1]</code></li>
<li><strong>置信度通道</strong>: 使用”expp1”激活 → <code>[B*S, H//down_ratio, W//down_ratio]</code></li>
</ul>
<h5 id="5-3-重新组织输出格式"><a href="#5-3-重新组织输出格式" class="headerlink" title="5.3 重新组织输出格式"></a>5.3 重新组织输出格式</h5><ul>
<li>深度图：<code>view(B, S, H//down_ratio, W//down_ratio, 1)</code></li>
<li>置信度图：<code>view(B, S, H//down_ratio, W//down_ratio)</code></li>
</ul>

  <p><a class="classtest-link" href="/tags/%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0/" rel="tag">代码学习</a>, <a class="classtest-link" href="/tags/%E8%AF%BB%E8%AE%BA%E6%96%87/" rel="tag">读论文</a> — Jul 10, 2025</p>
  


        </div>
        <div class="row mt-2">
  <h3>Search</h3>
  <div><input id="search-text" title="search" class="search-text" type="text" placeholder="search......"></div>
  <div style="margin-top: 1.5rem;">
    <ul id="result"></ul>
  </div>
</div>
        <div class="row mt-2">
  
    <div class="eight columns">
      <p id="madewith">Made with ❤ and
        <a class="footer-link icon" href="https://hexo.io" target="_blank" style="text-decoration: none;" rel="noreferrer" aria-label="Hexo.io">
        <svg class="hexo svg-hov" width="14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><title>Hexo.js</title><path d="M12 .007L1.57 6.056V18.05L12 23.995l10.43-6.049V5.952L12 .007zm4.798 17.105l-.939.521-.939-.521V12.94H9.08v4.172l-.94.521-.938-.521V6.89l.939-.521.939.521v4.172h5.84V6.89l.94-.521.938.521v10.222z"/></svg>
        </a>
        
    </div>

    <!-- Sepcial thanks to https://simpleicons.org/ for the icons -->
    <div class="four columns mb-3 posisi" >
      
      <a class="ml-0 footer-link icon" href="https://github.com/henu77" target="_blank" style="text-decoration: none" rel="noreferrer" aria-label="GitHub">
        <svg class="github svg-hov" width="18" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg>
      </a>
      

      

      

      

      

    </div>
  
</div>

      </div>

    </div>

  </div>
  <script src="/js/nanobar.min.js"></script>

  <script>
    var options = {
      classname: 'nanobar',
      id: 'myNanobar'
    };
    var nanobar = new Nanobar(options);
    nanobar.go(30);
    nanobar.go(76);
    nanobar.go(100);
  </script>

</body>

</html>