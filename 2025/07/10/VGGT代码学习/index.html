<!DOCTYPE html>
<html lang="en">

<head>

  <!-- Minima -->
  <!-- Hexo theme created by @adisaktijrs -->

  <!-- Basic Page Needs
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta charset="utf-8">

  
  <title>VGGT代码学习</title>
  
  <link rel="canonical" href="http://example.com/2025/07/10/VGGT%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0/">
  
  <meta name="description" content="VGGT: Visual Geometry Grounded Transformer （CVPR 2025 Best Paper）Code | Paper | Suplementary 1. 方法介绍 VGGT是一种创新的前馈神经网络，能够直接从单张、少量或上百张视图中推断出场景的全部关键3D属性，">
  
  
  <meta name="author" content="Malong">
  
  
  
  <meta property="og:site_name" content="Malong&#39;s Blog" />
  <meta property="og:type" content="article" />
  <meta property="og:title" content="VGGT代码学习" />
  
  <meta property="og:description" content="VGGT: Visual Geometry Grounded Transformer （CVPR 2025 Best Paper）Code | Paper | Suplementary 1. 方法介绍 VGGT是一种创新的前馈神经网络，能够直接从单张、少量或上百张视图中推断出场景的全部关键3D属性，">
  
  <meta property="og:url" content="http://example.com/2025/07/10/VGGT%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0/" />

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="VGGT代码学习">
  
  <meta name="twitter:description" content="VGGT: Visual Geometry Grounded Transformer （CVPR 2025 Best Paper）Code | Paper | Suplementary 1. 方法介绍 VGGT是一种创新的前馈神经网络，能够直接从单张、少量或上百张视图中推断出场景的全部关键3D属性，">
  
  
  
  
  <meta name="twitter:url" content="http://example.com/2025/07/10/VGGT%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0/" />

  <!-- Mobile Specific Metas
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <!-- Preload fonts
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="preload" href="/fonts/dm-serif-display-v4-latin-regular.woff2" as="font" type="font/woff2" crossorigin>
  <link rel="preload" href="/fonts/inter-v2-latin-regular.woff2" as="font" type="font/woff2" crossorigin>

  <!-- CSS
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  
<link rel="stylesheet" href="/css/normalize.css">

  
<link rel="stylesheet" href="/css/skeleton.css">

  
<link rel="stylesheet" href="/css/custom.css">

  
<link rel="stylesheet" href="/css/prism-dark.css">

  
<link rel="stylesheet" href="/css/prism-line-numbers.css">

  <!-- User css -->
  

  <!-- Favicon
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <link rel="icon" type="image/png" href="/images/favicon.png">

  <!-- Custom Theme Color Style
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  <style>
  a:not(.icon) {
    text-decoration-color: #0FA0CE;
    background-image: linear-gradient(
      to bottom,
      rgba(0, 0, 0, 0) 50%,
      #0FA0CE 50%
    );
  }
  blockquote {
    border-left: 8px solid #0FA0CE;
  }
  .nanobar .bar {
    background: #0FA0CE;
  }
  .button.button-primary:hover,
  button.button-primary:hover,
  input[type="submit"].button-primary:hover,
  input[type="reset"].button-primary:hover,
  input[type="button"].button-primary:hover,
  .button.button-primary:focus,
  button.button-primary:focus,
  input[type="submit"].button-primary:focus,
  input[type="reset"].button-primary:focus,
  input[type="button"].button-primary:focus {
    background-color: #0FA0CE;
    border-color: #0FA0CE;
  }
  input[type="email"]:focus,
  input[type="number"]:focus,
  input[type="search"]:focus,
  input[type="text"]:focus,
  input[type="tel"]:focus,
  input[type="url"]:focus,
  input[type="password"]:focus,
  textarea:focus,
  select:focus {
    border: 1px solid #0FA0CE;
  }
</style>

  <!-- Google Analytics (With Privacy Settings On)
  –––––––––––––––––––––––––––––––––––––––––––––––––– -->
  

  
  <script src="/js/pic.min.js" defer></script>
  

  

<meta name="generator" content="Hexo 7.3.0"></head>

<body>
  <div class="container">
    <div class="row">
      <div>

        <div class="row">
  <div class="two columns" style="max-width: 50px">
    <h1 class="mt-2 mode">
      <div onclick=setDarkMode(true) id="darkBtn">🌑</div>
      <div onclick=setDarkMode(false) id="lightBtn" class=hidden>☀️</div>
      <script >
        if (localStorage.getItem('preferredTheme') == 'dark') {
          setDarkMode(true)
        }
        function setDarkMode(isDark) {
          var darkBtn = document.getElementById('darkBtn')
          var lightBtn = document.getElementById('lightBtn')
          if (isDark) {
            lightBtn.style.display = "block"
            darkBtn.style.display = "none"
            localStorage.setItem('preferredTheme', 'dark');
          } else {
            lightBtn.style.display = "none"
            darkBtn.style.display = "block"
            localStorage.removeItem('preferredTheme');
          }
          document.body.classList.toggle("darkmode");
        }
      </script>
    </h1>
  </div>

  <div class="six columns ml-1">
    <h1 class="mt-2">
      Malong&#39;s Blog
    </h1>
  </div>

  <div class="twelve columns">
    <div class="row">
      <div class="nine columns left">
        <a href="/">Home</a>
        
          
          <a href="/Works" class="ml">Works</a>
          
        
        
          
            <a href="mailto:1289596706@qq.com" target="_blank" class="ml">Email</a>
          
        
      </div>
    </div>
    <hr style="margin-bottom: 2.6rem">
  </div>
</div>

        <div class="trans">
            <h2>VGGT代码学习</h2>

  <h1 id="VGGT-Visual-Geometry-Grounded-Transformer-（CVPR-2025-Best-Paper）"><a href="#VGGT-Visual-Geometry-Grounded-Transformer-（CVPR-2025-Best-Paper）" class="headerlink" title="VGGT: Visual Geometry Grounded Transformer （CVPR 2025 Best Paper）"></a>VGGT: Visual Geometry Grounded Transformer （CVPR 2025 Best Paper）</h1><p><a target="_blank" rel="noopener" href="https://github.com/facebookresearch/vggt">Code</a> | <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2025/papers/Wang_VGGT_Visual_Geometry_Grounded_Transformer_CVPR_2025_paper.pdf">Paper</a> | <a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/CVPR2025/supplemental/Wang_VGGT_Visual_Geometry_CVPR_2025_supplemental.pdf">Suplementary</a></p>
<h2 id="1-方法介绍"><a href="#1-方法介绍" class="headerlink" title="1. 方法介绍"></a>1. 方法介绍</h2><p><img src="/../images/image-1.png" alt="VGGT架构图"></p>
<p>VGGT是一种创新的前馈神经网络，能够直接从单张、少量或上百张视图中推断出场景的全部关键3D属性，包括相机参数、点图、深度图和3D点轨迹。</p>
<p>如上图所示，VGGT首先利用DINOv2从输入图像中提取Patch Tokens，并为每张图片添加一个随机初始化的Camera Token和Register Tokens。随后，将所有Patch Tokens、Register Tokens和Camera Token拼接，形成完整的输入序列。该序列被送入由Frame Attention和Global Attention组成的 交替注意力（Alternating-Attention，AA） 结构进行处理：Frame Attention专注于单张图像内部的Tokens，Global Attention则实现跨帧特征交互，关注所有图像间的Tokens。</p>
<p>在相机参数预测方面，模型通过4层额外的自注意力层和一个线性层，输出相机的内参和外参，参数化为旋转四元数、平移向量和视场角，统一为9维向量。对于点图和深度图，模型借助<a target="_blank" rel="noopener" href="https://openaccess.thecvf.com/content/ICCV2021/papers/Ranftl_Vision_Transformers_for_Dense_Prediction_ICCV_2021_paper.pdf">DPT</a>层将特征转换为密集特征图，再通过3×3和1×1卷积层输出深度图&#x2F;点图及其不确定性。3D点轨迹的预测则采用CoTracker2架构，将DPT提取的密集跟踪特征作为输入。具体而言，给定查询图像中的查询点，跟踪头能够预测所有帧中与该查询点对应的点集，实现高效的跨帧点追踪。</p>
<h2 id="2-代码学习"><a href="#2-代码学习" class="headerlink" title="2. 代码学习"></a>2. 代码学习</h2><p><img src="/../images/image-2.png" alt="VGGT类"></p>
<h3 id="2-1-Aggregator"><a href="#2-1-Aggregator" class="headerlink" title="2.1 Aggregator"></a>2.1 Aggregator</h3><p>Aggregator类是VGGT的核心组件，处理输入的多视角图片，提取特征并使用交替注意力来处理这些特征。</p>
<p>该模块可以分为3个主要部分：</p>
<ul>
<li>特征提取：使用DINOv2模型从输入图像中提取Patch Tokens，并且添加一个随机初始化的Camera Token和4个Register Token得到全部的特征序列。</li>
<li>位置编码：对特征序列使用RoPE生成Patch Tokens的二维位置编码，并为特殊Tokens生成0维位置编码。</li>
<li>交替注意力：使用Frame Attention和Global Attention来处理特征序列。（先进行Frame Attention之后进行Global Attention）两种注意力的实现都使用了RoPE位置编码。</li>
</ul>
<hr>
<h4 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h4><ul>
<li><code>images</code>：形状 <code>[B, S, 3, H, W]</code>，像素归一化到 [0, 1]，其中<ul>
<li><code>B</code> 是 batch size，</li>
<li><code>S</code> 是帧数，</li>
<li><code>3</code> 是 RGB 通道，</li>
<li><code>H</code> 和 <code>W</code> 分别是图片的高度和宽度。</li>
</ul>
</li>
</ul>
<h4 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h4><ul>
<li><p><strong>(a) 归一化</strong>  </p>
<ul>
<li>用 ImageNet 均值和方差对输入图片归一化。</li>
</ul>
</li>
<li><p><strong>(b) DINOv2特征提取</strong>  </p>
<ul>
<li>将图片 reshape 为 <code>[B*S, 3, H, W]</code>，输入 DINOv2中，得到 patch tokens，形状 <code>[B*S, P, C]</code>。</li>
<li>其中 <code>P</code> 是每帧的 patch 数量，<code>C</code> 是嵌入维度。</li>
</ul>
</li>
<li><p><strong>(c) 特殊 token 拼接</strong>  </p>
<ul>
<li>camera_token 和 register_token 通过 <code>slice_expand_and_flatten</code> 展开到 <code>[B*S, 1, C]</code> 和 <code>[B*S, num_register_tokens, C]</code>。</li>
<li>与 patch tokens 拼接，得到 <code>[B*S, 1+num_register_tokens+P, C]</code>。</li>
</ul>
</li>
<li><p><strong>(d) 位置编码</strong>  </p>
<ul>
<li>若启用 RoPE，生成 patch token 的二维位置编码，并为特殊 token 补零。</li>
</ul>
</li>
<li><p><strong>(e) 交替注意力堆叠</strong>  </p>
<ul>
<li>交替执行 frame attention 和 global attention，每种 attention 连续堆叠 aa_block_size 层。</li>
<li>frame attention：只在每帧内部做自注意力（tokens 形状 <code>[B*S, P, C]</code>）。</li>
<li>global attention：在所有帧的 token 间做自注意力（tokens reshape 为 <code>[B, S*P, C]</code>）。</li>
<li>每次堆叠后，frame 和 global attention 的输出分别保存为 intermediates。</li>
<li>每一组交替 attention 后，将 frame 和 global 的输出在最后一维拼接（<code>[B, S, P, 2C]</code>），加入 output_list。</li>
</ul>
</li>
<li><p><strong>(f) 输出</strong>  </p>
<ul>
<li>返回所有交替 attention 层的输出列表（每层形状 <code>[B, S, P, 2C]</code>），以及 patch_start_idx。</li>
</ul>
</li>
</ul>
<h3 id="2-2-Camera-Head"><a href="#2-2-Camera-Head" class="headerlink" title="2.2 Camera Head"></a>2.2 Camera Head</h3><p>Camera Head 负责从 Aggregator 输出的 token 表达中，预测每一帧的相机参数（如平移向量、旋转矩阵、视场角等），并采用迭代细化的方式提升预测精度。</p>
<hr>
<h4 id="输入-1"><a href="#输入-1" class="headerlink" title="输入"></a>输入</h4><ul>
<li><code>aggregated_tokens_list</code>：Aggregator 输出的 token 列表，取最后一层用于相机预测。</li>
<li><code>num_iterations</code>：迭代细化次数（默认 4）。</li>
</ul>
<h4 id="步骤-1"><a href="#步骤-1" class="headerlink" title="步骤"></a>步骤</h4><ul>
<li><p><strong>(a) 提取 camera token</strong>  </p>
<ul>
<li>从最后一层 token 中提取 camera token（通常为每帧的第一个 token），形状 <code>[B, S, C]</code>。</li>
<li>经过 LayerNorm 归一化。</li>
</ul>
</li>
<li><p><strong>(b) 迭代细化（<code>trunk_fn</code>）</strong>  </p>
<ul>
<li>初始化 <code>pred_pose_enc = None</code>。</li>
<li>重复 <code>num_iterations</code> 次，每次迭代细化预测，流程如下：<ol>
<li><strong>输入准备</strong>：<ul>
<li>第一次迭代，输入为 <code>empty_pose_tokens</code>，经 <code>embed_pose</code> 得到嵌入；</li>
<li>后续迭代，输入为上一次预测的 pose 编码（<code>detach</code> 后），经 <code>embed_pose</code> 得到嵌入。</li>
</ul>
</li>
<li><strong>调制参数生成</strong>：<ul>
<li>通过 <code>poseLN_modulation</code>，根据 pose 嵌入生成 shift、scale、gate 三组调制参数。</li>
</ul>
</li>
<li><strong>自适应归一化与调制</strong>：<ul>
<li>对 camera token 先做 <code>adaln_norm</code>，再用 shift&#x2F;scale&#x2F;gate 进行调制（modulate）。</li>
</ul>
</li>
<li><strong>主干网络处理</strong>：<ul>
<li>将调制后的 token 输入 Transformer Block（trunk），提取特征。</li>
</ul>
</li>
<li><strong>增量预测</strong>：<ul>
<li>trunk 输出经归一化（trunk_norm）后，输入 <code>pose_branch</code>（MLP），得到本次的 pose 增量（delta）。</li>
</ul>
</li>
<li><strong>累加与激活</strong>：<ul>
<li>将 delta 累加到 pred_pose_enc 上，得到当前 pose 编码；</li>
<li>对平移向量、四元数、FOV 分别用指定激活函数处理，确保物理合理性。</li>
</ul>
</li>
<li><strong>结果保存</strong>：<ul>
<li>每次迭代的 pose 编码都保存到列表，便于后续分析与可视化。</li>
</ul>
</li>
</ol>
</li>
</ul>
</li>
<li><p><strong>(c) 输出</strong>  </p>
<ul>
<li>返回每次迭代的 pose 编码列表。</li>
</ul>
</li>
</ul>
<h4 id="关键点说明"><a href="#关键点说明" class="headerlink" title="关键点说明"></a>关键点说明</h4><ul>
<li><strong>迭代 refinement</strong>：通过多次 refinement，逐步细化相机参数预测，提升精度和稳定性。</li>
<li><strong>自适应归一化与调制</strong>：借鉴 DiT 等模型，用 shift&#x2F;scale&#x2F;gate 对 token 做调制，增强表达能力。</li>
<li><strong>激活函数</strong>：对不同类型参数采用不同激活，保证物理合理性（如 FOV 必须为正，四元数归一化等）。</li>
</ul>
<h3 id="2-3-Depth-Head"><a href="#2-3-Depth-Head" class="headerlink" title="2.3 Depth Head"></a>2.3 Depth Head</h3><p>Depth Head用于从 Aggregator 输出的 token 表达中，预测每一帧的深度图和不确定性。</p>
<hr>
<h4 id="输入-2"><a href="#输入-2" class="headerlink" title="输入"></a>输入</h4><ul>
<li><code>aggregated_tokens_list</code>: Aggregator输出的多层token特征列表，每层形状为 <code>[B, S, P + 1 + num_register_tokens, 2*C]</code></li>
<li><code>images</code>: 原始输入图像，形状为 <code>[B, S, 3, H, W]</code>，像素值范围 [0, 1]</li>
<li><code>patch_start_idx</code>: patch token起始索引，通常为 <code>1 + num_register_tokens</code></li>
</ul>
<h4 id="步骤-2"><a href="#步骤-2" class="headerlink" title="步骤"></a>步骤</h4><ul>
<li><p><strong>(a) 多层特征提取（4个中间层）</strong><br>对于 <code>intermediate_layer_idx</code> 指定的4个层（默认为第4、11、17、23层），依次进行以下处理：</p>
<ul>
<li><p>a.1 提取patch token</p>
<ul>
<li>从 <code>aggregated_tokens_list[layer_idx]</code> 中提取patch部分：<code>[:, :, patch_start_idx:]</code></li>
<li>输入形状：<code>[B, S, P + 1 + num_register_tokens, 2*C]</code></li>
<li>输出形状：<code>[B, S, P, 2*C]</code></li>
</ul>
</li>
<li><p>a.2 重新组织张量</p>
<ul>
<li>将batch和序列维度合并：<code>reshape(B * S, P, 2*C)</code></li>
<li>对token进行LayerNorm归一化</li>
<li>重排为空间格式：<code>(B*S, 2*C, patch_h, patch_w)</code><ul>
<li><code>patch_h = H // patch_size</code></li>
<li><code>patch_w = W // patch_size</code></li>
</ul>
</li>
</ul>
</li>
<li><p>a.3 通道投影</p>
<ul>
<li>通过1x1卷积投影到指定通道数：<code>projects[dpt_idx](x)</code></li>
<li>输出形状：<code>[B*S, out_channels[dpt_idx], patch_h, patch_w]</code><ul>
<li>其中 out_channels &#x3D; [256, 512, 1024, 1024]</li>
</ul>
</li>
</ul>
</li>
<li><p>a.4 UV位置编码</p>
<ul>
<li>创建UV网格：<code>[patch_h, patch_w, 2]</code></li>
<li>转换为位置embedding：<code>[patch_h, patch_w, out_channels[dpt_idx]]</code></li>
<li>与特征相加：<code>x + pos_embed</code></li>
</ul>
</li>
<li><p>a.5 多尺度空间变换<br>通过 <code>resize_layers</code> 生成4个不同尺度的特征：</p>
<ul>
<li><strong>Layer 0</strong>: ConvTranspose2d(kernel&#x3D;4, stride&#x3D;4) → <code>[B*S, 256, patch_h*4, patch_w*4]</code></li>
<li><strong>Layer 1</strong>: ConvTranspose2d(kernel&#x3D;2, stride&#x3D;2) → <code>[B*S, 512, patch_h*2, patch_w*2]</code>  </li>
<li><strong>Layer 2</strong>: Identity() → <code>[B*S, 1024, patch_h, patch_w]</code></li>
<li><strong>Layer 3</strong>: Conv2d(kernel&#x3D;3, stride&#x3D;2) → <code>[B*S, 1024, patch_h//2, patch_w//2]</code></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>(b) 特征融合（scratch forward）</strong><br>scratch是一个类UNet的特征融合模块，用于融合上一步产生的多尺度特征。它通过自底向上的方式逐层融合不同尺度的特征，增强模型对空间细节的捕捉能力。</p>
<ul>
<li>b.1 通道统一</li>
</ul>
</li>
</ul>
<p>将4个尺度的特征通过3x3卷积（<strong>纯卷积</strong>）统一为相同通道数（256）：<br>    - layer_1_rn: <code>[B*S, 256, patch_h*4, patch_w*4]</code><br>    - layer_2_rn: <code>[B*S, 256, patch_h*2, patch_w*2]</code><br>    - layer_3_rn: <code>[B*S, 256, patch_h, patch_w]</code><br>    - layer_4_rn: <code>[B*S, 256, patch_h//2, patch_w//2]</code></p>
<ul>
<li><p>b.2 自底向上融合<br>特征融合采用FeatureFusionBlock模块，逐层自底向上整合多尺度特征。该模块支持跳跃连接（skip connection），以增强不同层级特征的交互。</p>
<ul>
<li><p><strong>不使用跳跃连接</strong>：仅输入x1，经过两层带残差的卷积后直接上采样。</p>
</li>
<li><p><strong>使用跳跃连接</strong>：输入为x1和x2。首先对x1进行两层带残差的卷积处理，然后将x2作为残差项加到x1上。之后再次通过两层带残差的卷积，最后进行上采样操作。</p>
</li>
<li><p><strong>具体流程如下</strong>：</p>
<ol>
<li><p><strong>refinenet4</strong>: 处理最小尺度特征，没有跳跃连接</p>
<ul>
<li>输入：layer_4_rn <code>[B*S, 256, patch_h//2, patch_w//2]</code></li>
<li>上采样到layer_3尺度：<code>[B*S, 256, patch_h, patch_w]</code></li>
</ul>
</li>
<li><p><strong>refinenet3</strong>: 融合第3层，</p>
<ul>
<li>输入：上步输出 + layer_3_rn</li>
<li>上采样到layer_2尺度：<code>[B*S, 256, patch_h*2, patch_w*2]</code></li>
</ul>
</li>
<li><p><strong>refinenet2</strong>: 融合第2层</p>
<ul>
<li>输入：上步输出 + layer_2_rn  </li>
<li>上采样到layer_1尺度：<code>[B*S, 256, patch_h*4, patch_w*4]</code></li>
</ul>
</li>
<li><p><strong>refinenet1</strong>: 融合第1层</p>
<ul>
<li>输入：上步输出 + layer_1_rn</li>
<li>最终上采样：<code>[B*S, 256, patch_h*8, patch_w*8]</code></li>
</ul>
</li>
<li><p><strong>输出层</strong>：经过3x3卷积，输出通道减半</p>
<ul>
<li>输出形状：<code>[B*S, 128, patch_h*8, patch_w*8]</code></li>
</ul>
</li>
</ol>
</li>
</ul>
<p>这种自底向上的多层级融合方式，能够充分整合不同语义层次的特征信息，有效提升深度预测的空间细节和全局一致性。</p>
</li>
<li><p><strong>(c) 空间分辨率恢复</strong></p>
<ul>
<li>使用双线性插值上采样到目标分辨率：</li>
<li>输入：<code>[B*S, 128, patch_h*8, patch_w*8]</code></li>
<li>输出：<code>[B*S, 128, H//down_ratio, W//down_ratio]</code>（通常down_ratio&#x3D;1）</li>
</ul>
</li>
<li><p><strong>(d) 再次位置编码</strong></p>
<ul>
<li>再次添加UV位置编码增强空间感知</li>
</ul>
</li>
<li><p><strong>(e) 最终输出处理</strong></p>
<ul>
<li><code>output_conv1</code>: 3x3卷积 + ReLU + 1x1卷积<ul>
<li>输出形状：<code>[B*S, 2, H//down_ratio, W//down_ratio]</code></li>
<li>其中2个通道分别表示深度图和置信度图。</li>
</ul>
</li>
<li><code>activate_head</code>: 分离并激活两个通道<ul>
<li>深度通道使用”exp”激活，输出形状 <code>[B*S, H//down_ratio, W//down_ratio, 1]</code></li>
<li>置信度通道使用”expp1”激活，输出形状 <code>[B*S, H//down_ratio, W//down_ratio]</code></li>
</ul>
</li>
<li>重新组织输出格式：<ul>
<li>深度图：<code>view(B, S, H//down_ratio, W//down_ratio, 1)</code></li>
<li>置信度图：<code>view(B, S, H//down_ratio, W//down_ratio)</code></li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="2-4-Point-Head"><a href="#2-4-Point-Head" class="headerlink" title="2.4 Point Head"></a>2.4 Point Head</h3><p>Point Head与Depth Head类似，负责从 Aggregator 输出的 token 表达中，预测每一帧的点图和不确定性。两个预测头结构一致但是最终输出的结果一个为3通道点图一个为1通道深度图。</p>
<h3 id="2-5-Track-Head"><a href="#2-5-Track-Head" class="headerlink" title="2.5 Track Head"></a>2.5 Track Head</h3><p>Track Head用于从 Aggregator 输出的 token 表达中，预测查询点在视频序列中的轨迹、可见性和置信度。</p>
<hr>
<h4 id="输入-3"><a href="#输入-3" class="headerlink" title="输入"></a>输入</h4><ul>
<li><code>aggregated_tokens_list</code>: Aggregator输出的多层token特征列表，每层形状为 <code>[B, S, P + 1 + num_register_tokens, 2*C]</code></li>
<li><code>images</code>: 原始输入图像，形状为 <code>[B, S, 3, H, W]</code>，像素值范围 [0, 1]</li>
<li><code>patch_start_idx</code>: patch token起始索引，通常为 <code>1 + num_register_tokens</code></li>
<li><code>query_points</code>: 查询点像素坐标，形状为 <code>[B, N, 2]</code>，其中N为查询点数量</li>
</ul>
<h4 id="步骤-3"><a href="#步骤-3" class="headerlink" title="步骤"></a>步骤</h4><ul>
<li><p><strong>(a) 特征提取（基于DPT架构）</strong><br>Track Head使用DPT架构的特征提取器，配置为仅输出特征（<code>feature_only=True</code>）：</p>
<ul>
<li><p>a.1 多层特征提取</p>
<ul>
<li>与Depth Head类似，从4个中间层提取token特征</li>
<li>每层处理：提取patch token → LayerNorm → 重组空间格式 → 通道投影 → UV位置编码</li>
</ul>
</li>
<li><p>a.2 多尺度空间变换</p>
<ul>
<li><strong>Layer 0</strong>: ConvTranspose2d(kernel&#x3D;4, stride&#x3D;4) → <code>[B*S, 256, patch_h*4, patch_w*4]</code></li>
<li><strong>Layer 1</strong>: ConvTranspose2d(kernel&#x3D;2, stride&#x3D;2) → <code>[B*S, 512, patch_h*2, patch_w*2]</code></li>
<li><strong>Layer 2</strong>: Identity() → <code>[B*S, 1024, patch_h, patch_w]</code></li>
<li><strong>Layer 3</strong>: Conv2d(kernel&#x3D;3, stride&#x3D;2) → <code>[B*S, 1024, patch_h//2, patch_w//2]</code></li>
</ul>
</li>
<li><p>a.3 特征融合（scratch forward）</p>
<ul>
<li>通过FeatureFusionBlock逐层融合多尺度特征</li>
<li>自底向上融合：refinenet4 → refinenet3 → refinenet2 → refinenet1</li>
<li>最终输出：<code>[B*S, 128, patch_h*8, patch_w*8]</code></li>
</ul>
</li>
<li><p>a.4 空间分辨率调整</p>
<ul>
<li>使用双线性插值调整到目标分辨率：<code>[B*S, 128, H//2, W//2]</code>（down_ratio&#x3D;2）</li>
<li>重新组织为序列格式：<code>[B, S, 128, H//2, W//2]</code></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>(b) 迭代跟踪预测</strong><br>使用BaseTrackerPredictor进行迭代优化的点跟踪：</p>
<ul>
<li><p>b.1 初始化阶段</p>
<ul>
<li><strong>查询点处理</strong>: 如果未提供query_points，tracker自动初始化</li>
<li><strong>特征金字塔构建</strong>: 构建相关性金字塔用于特征匹配</li>
<li><strong>坐标初始化</strong>: 设置初始跟踪坐标</li>
</ul>
</li>
<li><p>b.2 迭代优化循环（默认4次迭代）<br>对于每次迭代 <code>iter in range(iters)</code>：</p>
<ul>
<li><p><strong>特征采样</strong>: 在当前坐标位置对特征图进行双线性采样</p>
<ul>
<li>输入：feature_maps <code>[B, S, 128, H//2, W//2]</code>，current_coords <code>[B, S, N, 2]</code></li>
<li>输出：sampled_features <code>[B, S, N, 128]</code></li>
</ul>
</li>
<li><p><strong>相关性计算</strong>: 计算查询特征与目标特征的相关性</p>
<ul>
<li>构建相关性金字塔（7层，半径4）</li>
<li>输出：correlation_features <code>[B, S, N, corr_levels*corr_radius*2+1]</code></li>
</ul>
</li>
<li><p><strong>运动估计</strong>: 基于相关性和上下文特征预测坐标更新</p>
<ul>
<li>输入：correlation_features + context_features</li>
<li>通过GRU网络处理：<code>hidden_size=384</code></li>
<li>输出：delta_coords <code>[B, S, N, 2]</code></li>
</ul>
</li>
<li><p><strong>坐标更新</strong>: 应用预测的偏移量</p>
<ul>
<li><code>current_coords = current_coords + delta_coords</code></li>
</ul>
</li>
<li><p><strong>可见性预测</strong>: 预测点在当前帧的可见性</p>
<ul>
<li>输入：sampled_features</li>
<li>输出：visibility_scores <code>[B, S, N]</code>（范围0-1）</li>
</ul>
</li>
<li><p><strong>置信度预测</strong>: 预测跟踪的置信度（如果enabled）</p>
<ul>
<li>输入：sampled_features + correlation_features</li>
<li>输出：confidence_scores <code>[B, S, N]</code>（范围0-1）</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>(c) 输出整理</strong></p>
<ul>
<li><p>c.1 轨迹坐标列表</p>
<ul>
<li>收集所有迭代的坐标预测：<code>coord_preds = [iter0_coords, iter1_coords, ..., iter3_coords]</code></li>
<li>每个元素形状：<code>[B, S, N, 2]</code>（像素坐标）</li>
</ul>
</li>
<li><p>c.2 可见性分数</p>
<ul>
<li>最终迭代的可见性预测：<code>vis_scores [B, S, N]</code></li>
</ul>
</li>
<li><p>c.3 置信度分数</p>
<ul>
<li>最终迭代的置信度预测：<code>conf_scores [B, S, N]</code></li>
</ul>
</li>
</ul>
</li>
</ul>

  <p><a class="classtest-link" href="/tags/%E4%BB%A3%E7%A0%81%E5%AD%A6%E4%B9%A0/" rel="tag">代码学习</a>, <a class="classtest-link" href="/tags/%E8%AF%BB%E8%AE%BA%E6%96%87/" rel="tag">读论文</a> — Jul 10, 2025</p>
  


        </div>
        <div class="row mt-2">
  <h3>Search</h3>
  <div><input id="search-text" title="search" class="search-text" type="text" placeholder="search......"></div>
  <div style="margin-top: 1.5rem;">
    <ul id="result"></ul>
  </div>
</div>
        <div class="row mt-2">
  
    <div class="eight columns">
      <p id="madewith">Made with ❤ and
        <a class="footer-link icon" href="https://hexo.io" target="_blank" style="text-decoration: none;" rel="noreferrer" aria-label="Hexo.io">
        <svg class="hexo svg-hov" width="14" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><title>Hexo.js</title><path d="M12 .007L1.57 6.056V18.05L12 23.995l10.43-6.049V5.952L12 .007zm4.798 17.105l-.939.521-.939-.521V12.94H9.08v4.172l-.94.521-.938-.521V6.89l.939-.521.939.521v4.172h5.84V6.89l.94-.521.938.521v10.222z"/></svg>
        </a>
        
    </div>

    <!-- Sepcial thanks to https://simpleicons.org/ for the icons -->
    <div class="four columns mb-3 posisi" >
      
      <a class="ml-0 footer-link icon" href="https://github.com/henu77" target="_blank" style="text-decoration: none" rel="noreferrer" aria-label="GitHub">
        <svg class="github svg-hov" width="18" role="img" viewBox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><title>GitHub</title><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"/></svg>
      </a>
      

      

      

      

      

    </div>
  
</div>

      </div>

    </div>

  </div>
  <script src="/js/nanobar.min.js"></script>

  <script>
    var options = {
      classname: 'nanobar',
      id: 'myNanobar'
    };
    var nanobar = new Nanobar(options);
    nanobar.go(30);
    nanobar.go(76);
    nanobar.go(100);
  </script>

</body>

</html>